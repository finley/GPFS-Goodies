#!/usr/bin/perl -w

#
#   "tune_block_device_settings"
#
#   How it works:
#
#       This command should be run from one of your GPFS cluster
#       connected nodes after creating your GPFS file systems.  It will
#       detect a number of things in your GPFS cluster environment, and
#       make calculations based on best practices to create optimized
#       block device tuning settings in the form of udev rules.
#
#       Some of the data it uses to make it's calculations include:
#
#           - Storage controller models
#           - Storage controller settings, such as cache mirroring
#             enabled/disabled
#           - Number of storage servers connected to each controller
#           - GPFS file system block sizes
#
#       It will create a single udev rule file per file system
#       (/etc/udev/rules.d/99-gpfs-FSNAME.rules), with one rule per
#       device and give you the option of a) deploying the rules to your
#       storage servers for you, and b) hot-activating the settings via
#       udev.
#

#
#   2013.11.10 Brian Elliott Finley <bfinley@us.ibm.com>
#   - created
#   2013.11.18 Brian Elliott Finley <bfinley@us.ibm.com>
#   - add initial default tuning settings
#   2014.01.03 Brian Elliott Finley <bfinley@us.ibm.com>
#   - minor refinements after testing on Ray Paden's test cluster
#   2014.08.06 Brian Elliott Finley <bfinley@us.ibm.com>
#   - handle tiered file systems, and other file systems with multiple 
#     pools gracefully
#   - detect GSS file systems and exit with no action taken
#   - Operate in "--test" mode if "--deploy" not specified.  Suggested
#     by Francis Dang <francis@tamu.edu>.
#

use strict;
use Getopt::Long;
use File::Basename;

my $progname = basename($0);
my $version_number = '20.8.7';
my $disks_per_array_default = 8;
my $udev_rules_file_base = "/etc/udev/rules.d/99-gpfs_goodies";

our $ERROR = 0;

GetOptions(
    "help"                      => \my $help,
    "version"                   => \my $version,
    "filesystem|fsname|fs=s"    => \my $fs,
    "deploy"                    => \my $deploy,
    "disks-per-array=s"         => \my $disks_per_array,
    "test|test-run"             => \my $test_run,
    "verbose|v"                 => \my $debug,
    "out-file=s"                => \my $out_file,
) or usage() and exit(1);

if ( defined $help ) {
    usage();
    exit 0;
} 
elsif ( defined $version ) {
    version();
    exit 0;
}

if( defined $deploy and defined $test_run ) {
    usage();
    print qq(\n-->  Try either "--deploy" or "--test"\n\n);
    exit 0;
}

if( ! defined $fs ) {
    usage();
    print qq(\n-->  Try "--filesystem FSNAME"\n\n);
    exit 0;
}

if( ! defined $deploy and ! defined $test_run ) {
    $test_run = 1;
    print qq(Running in "--test" mode only -- no changes will be made to your system.\n);
}

if( ! defined $out_file ) {
    $out_file = get_tmp_file();
}

if ( ! user_is_root() ) {
    usage();
    print qq(\n-->  Please run as root\n\n);
    exit 1;
}

unless ( -e "/usr/lpp/mmfs/bin/mmlsnsd" and -e "/usr/lpp/mmfs/bin/mmlsfs" ) {
    usage();
    print qq(\n-->  GPFS doesn't appear to be installed on this machine\n\n);
    exit 1;
}

my %servers;
my $block_size;
my %server_by_disk_path;
my $block_allocation_type;
my %disk_path_by_disk_name;
my %disk_pools;
my %disk_pool_by_disk_name;
my %disks_per_array_by_pool;


check_for_fs();
get_disk_pool_info();
process_disks_per_array_option();
get_mmlsnsd_info();
detect_gss_servers();
get_mmlsfs_info();
create_udev_rules();
report_results();

exit 0;



########################################################################
#
#   BEGIN Subroutines
#
sub process_disks_per_array_option {

    my $valid;

    # Pre-assign the default for each pool
    foreach my $pool (keys %disk_pools) {
        $disks_per_array_by_pool{$pool} = $disks_per_array_default;
        $valid = 'yes';
    }
    
    if( defined $disks_per_array ) {
    
        #
        # Override the default with custom setting
        #
        if( $disks_per_array =~ m/^\d+$/ ) {

            foreach my $pool (keys %disk_pools) {
                $disks_per_array_by_pool{$pool} = $disks_per_array;
                $valid = 'yes';
            }

        } elsif( $disks_per_array =~ m/:/ ) {

            my @arguments = split(/,/, $disks_per_array);
            foreach (@arguments) {

                my ($count, $pool) = split(/:/);

                #
                # Break out of the loop and jump to the error message unless
                # $count is a digit.
                #
                unless($count =~ m/^\d+$/) {
                    $valid = 'no';
                    last;
                }

                $disks_per_array_by_pool{$pool} = $count;
                $valid = 'yes';
            }
        }
    }


    if($debug) {
        foreach my $pool (sort keys %disks_per_array_by_pool) {
            print "\$disks_per_array_by_pool{$pool}: $disks_per_array_by_pool{$pool}\n";
        }
    }


    if($valid eq 'yes') {
        return 1;
    } else {
        usage();
        print qq(\n-->  "$disks_per_array" is an invalid value for --disks-per-array.\n\n);
        exit 1;
    }
}


sub check_for_fs {

    my $cmd = "mmlsfs $fs >/dev/null 2>&1";
    if( !system($cmd) ) {
        # success
        print qq(Examining file system "$fs"...\n);
    } else {
        # fail
        print "\n";
        print qq(WARNING:  I can't seem to find file system "$fs".  Please make sure to:\n);
        print "\n";
        print "  - check the spelling of the file system name\n";
        print qq(  - run this command from a node in the GPFS cluster that owns "$fs"\n);
        print "\n";

        exit 1;
    }
}


sub detect_gss_servers {

    print "> detect_gss_servers()\n" if(defined $debug);

    my @gss_servers;

    foreach my $server (sort keys %servers) {

        next unless($servers{$server} eq 'suspect');

        my $cmd = qq(ssh $server rpm -q gpfs.gss.firmware >/dev/null 2>&1);
        print ">> Command: $cmd\n" if(defined $debug);
        if( !system($cmd) ) {
            push @gss_servers, $server;
        }
    }

    if(@gss_servers) {

        print qq(\n); 
        print qq(WARNING:\n); 
        print qq(\n); 
        print qq(  The following servers for file system "$fs" appear to be GSS servers:\n);
        print qq(\n); 

        foreach my $server (@gss_servers) {
            print "    $server\n";
        }

        print qq(\n); 
        print qq(  This tool is not necessary for GSS file systems, so I'm exiting\n);
        print qq(  now without making any changes.\n);
        print qq(\n); 

        exit 1;
    }
}


sub get_mmlsnsd_info {

    print "> get_mmlsnsd_info()\n" if(defined $debug);
    my $cmd = "/usr/lpp/mmfs/bin/mmlsnsd -f $fs -m 2>&1";
    print ">> Command: $cmd\n" if(defined $debug);
    open(INPUT,"$cmd|") or die("Couldn't run $cmd for input.");
    while(<INPUT>) {
        #	
        #   Sample input:
        #
        #	[root@x36n10 bef]# mmlsnsd -m
        #	
        #	 Disk name    NSD volume ID      Device         Node name                Remarks       
        #	---------------------------------------------------------------------------------------
        #	 nsd_0        AC100009527FEBFE   /dev/mapper/dcs3860a_lun0 x36n09                   server node
        #	 nsd_0        AC100009527FEBFE   /dev/mapper/dcs3860a_lun0 x36n10                   server node
        #	 nsd_0        AC100009527FEBFE   /dev/mapper/dcs3860a_lun0 x36n11                   
        #	 nsd_0        AC100009527FEBFE   /dev/mapper/dcs3860a_lun0 x36n12                   
        #	 nsd_1        AC10000A527FEC03   /dev/mapper/dcs3860b_lun1 x36n09                   
        #	 nsd_1        AC10000A527FEC03   /dev/mapper/dcs3860b_lun1 x36n10                   server node
        #	 nsd_1        AC10000A527FEC03   /dev/mapper/dcs3860b_lun1 x36n11                   server node
        #	 nsd_1        AC10000A527FEC03   /dev/mapper/dcs3860b_lun1 x36n12                   
        #	 nsd_10       AC10000A527FEC30   /dev/mapper/dcs3860a_lun11 x36n09                   
        #	 nsd_10       AC10000A527FEC30   /dev/mapper/dcs3860a_lun11 x36n10                   server node
        #	 nsd_10       AC10000A527FEC30   /dev/mapper/dcs3860a_lun11 x36n11                   server node
        #	 nsd_10       AC10000A527FEC30   /dev/mapper/dcs3860a_lun11 x36n12                   
        #	 nsd_11       AC10000B527FEC34   /dev/mapper/dcs3860b_lun10 x36n09                   
        #	 nsd_11       AC10000B527FEC34   /dev/mapper/dcs3860b_lun10 x36n10                   
        #	 nsd_11       AC10000B527FEC34   /dev/mapper/dcs3860b_lun10 x36n11                   server node
        #	 nsd_11       AC10000B527FEC34   /dev/mapper/dcs3860b_lun10 x36n12                   server node
        #	 nsd_12       AC10000B527FEC39   /dev/mapper/dcs3860a_lun12 x36n09                   
        #	 nsd_12       AC10000B527FEC39   /dev/mapper/dcs3860a_lun12 x36n10                   
        #	[snip]
        #
        #	[root@flashnsd1 bef]# mmlsnsd -m
        #    Disk name    NSD volume ID      Device         Node name                Remarks       
        #   ---------------------------------------------------------------------------------------
        #    flashctl1ab_lun0 0A46011F53BD6EC5   /dev/mapper/flashctl1ab_lun0 flashnsd1-10g            server node
        #    flashctl1ab_lun0 0A46011F53BD6EC5   /dev/mapper/flashctl1ab_lun0 flashnsd2-10g            server node
        #    flashctl1ab_lun1 0A46011F53BD6ECA   /dev/mapper/flashctl1ab_lun1 flashnsd1-10g            server node
        #    flashctl1ab_lun1 0A46011F53BD6ECA   /dev/mapper/flashctl1ab_lun1 flashnsd2-10g            server node
        #    flashctl1ab_lun2 0A46012053BD6ECF   /dev/mapper/flashctl1ab_lun2 flashnsd1-10g            server node
        #    flashctl1ab_lun2 0A46012053BD6ECF   /dev/mapper/flashctl1ab_lun2 flashnsd2-10g            server node
        #    flashctl1ab_lun3 0A46012053BD6ED4   /dev/mapper/flashctl1ab_lun3 flashnsd1-10g            server node
        #    flashctl1ab_lun3 0A46012053BD6ED4   /dev/mapper/flashctl1ab_lun3 flashnsd2-10g            server node
        #    gss1_Data_16M_2p_1 0A460501539A6825   gss1_Data_16M_2p_1 gss1-10g.cluster         server node
        #    gss1_Data_16M_2p_1 0A460501539A6825   gss1_Data_16M_2p_1 gss2-10g.cluster         server node
        #    gss1_Data_16M_2p_2 0A460501539A6838   gss1_Data_16M_2p_2 gss1-10g.cluster         server node
        #    gss1_Data_16M_2p_2 0A460501539A6838   gss1_Data_16M_2p_2 gss2-10g.cluster         server node
        #    gss1_Data_16M_2p_3 0A460501539A684B   gss1_Data_16M_2p_3 gss1-10g.cluster         server node
        #    gss1_Data_16M_2p_3 0A460501539A684B   gss1_Data_16M_2p_3 gss2-10g.cluster         server node
        #    gss1_Data_4M_2p_1 0A460501539A68BC   gss1_Data_4M_2p_1 gss1-10g.cluster         server node
        #    gss1_Data_4M_2p_1 0A460501539A68BC   gss1_Data_4M_2p_1 gss2-10g.cluster         server node
        #    gss1_Data_4M_2p_2 0A460501539A68C6   gss1_Data_4M_2p_2 gss1-10g.cluster         server node
        #    gss1_Data_4M_2p_2 0A460501539A68C6   gss1_Data_4M_2p_2 gss2-10g.cluster         server node
        #    gss1_Data_4M_2p_3 0A460501539A68D0   gss1_Data_4M_2p_3 gss1-10g.cluster         server node
        #    gss1_Data_4M_2p_3 0A460501539A68D0   gss1_Data_4M_2p_3 gss2-10g.cluster         server node
        #   [snip]
        #
        print ">> $_" if(defined $debug);
        if(m/^\s+(\S+)\s+\S+\s+(\S+)\s+(\S+)\s.*server node$/) {
    
            my $disk_name   = $1;
            my $disk_path   = $2;
            my $server      = $3;

            #
            #   Note that the GSS NSD devices don't start with /dev/, and therefore
            #   will not be processed.  We will silently and harmlessly skip over
            #   such disk devices, mark those servers as suspect, and test them
            #   later to see if they are in fact GSS nodes. -BEF-
            #
            if( $disk_path =~ m#^/dev/# ) {
                $disk_path_by_disk_name{$disk_name} = $disk_path;
                $server_by_disk_path{$disk_path} = $server;
                $servers{$server} = 'nsd';
            } else {
                $servers{$server} = 'suspect';
            }
        }
    }
    close(INPUT);

    return 1;
}


sub get_mmlsfs_info {

    print "> get_mmlsfs_info()\n" if(defined $debug);
    my $cmd = "/usr/lpp/mmfs/bin/mmlsfs $fs 2>&1";
    print ">> Command: $cmd\n" if(defined $debug);
    open(INPUT,"$cmd|") or die("Can't run $cmd for input");
    while(<INPUT>) {
        #
        # Sample input:
        #
        # [root@x36n01 bef]# ./tune_block_device_settings 
        # flag                value                    description
        # ------------------- ------------------------ -----------------------------------
        #  -f                 32768                    Minimum fragment size in bytes
        #  -i                 512                      Inode size in bytes
        #  -I                 32768                    Indirect block size in bytes
        #  -m                 1                        Default number of metadata replicas
        #  -M                 2                        Maximum number of metadata replicas
        #  -r                 1                        Default number of data replicas
        #  -R                 2                        Maximum number of data replicas
        #  -j                 scatter                  Block allocation type
        #  -D                 nfs4                     File locking semantics in effect
        #  -k                 all                      ACL semantics in effect
        #  -n                 32                       Estimated number of nodes that will mount file system
        #  -B                 1048576                  Block size
        #  -Q                 none                     Quotas enforced
        #                     none                     Default quotas enabled
        #  --filesetdf        No                       Fileset df enabled?
        #  -V                 13.23 (3.5.0.7)          File system version
        #  --create-time      Sun Nov 10 17:20:44 2013 File system creation time
        #  -u                 Yes                      Support for large LUNs?
        #  -z                 No                       Is DMAPI enabled?
        #  -L                 4194304                  Logfile size
        #  -E                 Yes                      Exact mtime mount option
        #  -S                 No                       Suppress atime mount option
        #  -K                 whenpossible             Strict replica allocation option
        #  --fastea           Yes                      Fast external attributes enabled?
        #  --inode-limit      134217728                Maximum number of inodes
        #  -P                 system                   Disk storage pools in file system
        #  -d                 nsd_0;nsd_1;nsd_2;nsd_3;nsd_4;nsd_5;nsd_6;nsd_7;nsd_8;nsd_9;nsd_10;nsd_11;nsd_12;nsd_13;nsd_14;nsd_15;nsd_16;nsd_17;nsd_18;nsd_19;nsd_20;nsd_21;nsd_22;nsd_23;nsd_24;
        #  -d                 nsd_25;nsd_26;nsd_27;nsd_28;nsd_29  Disks in file system
        #  --perfileset-quota no                       Per-fileset quota enforcement
        #  -A                 yes                      Automatic mount option
        #  -o                 none                     Additional mount options
        #  -T                 /fs_1m                   Default mount point
        #  --mount-priority   0                        Mount priority
        # 
        print ">> $_" if(defined $debug);

        if(m/^\s+-B\s+(\d+)\s+/) {
            #  -B                 1048576                  Block size
            $block_size = $1;

        } elsif(m/^\s+-j\s+(scatter)\s+/) {
            #  -j                 scatter                  Block allocation type
            $block_allocation_type = $1;
        }
    }
    close(INPUT);

    return 1;
}


sub create_udev_rules {

    print qq(Creating fresh udev rules for file system "$fs"...\n);

    my $file = $out_file;
    open(FILE,">$file") or die("Couldn't open $file for writing");

    #
    # Create LUN specific settings
    #
    foreach my $disk_name (keys %disk_path_by_disk_name) {
    
        my $disk_path   = $disk_path_by_disk_name{$disk_name};
        my $server      = $server_by_disk_path{$disk_path};
    
        my $max_sectors_kb  = calculate_max_sectors_kb($block_size);
        my $read_ahead_kb   = calculate_read_ahead_kb($block_allocation_type, $max_sectors_kb);
        my $nr_requests     = calculate_nr_requests($disk_name);
        my $queue_depth     = calculate_queue_depth($disk_name);
        my $elevator        = 'noop';
    
        my $scsi_id = get_scsi_id( $server, $disk_path );
        if(defined $scsi_id) {
    
            print FILE qq(#\n);
            print FILE qq(# NSD: $disk_name  PATH: $disk_path\n);
            print FILE qq(#\n);
            print FILE qq(#  Individual Devices\n);
            print FILE q(SUBSYSTEM=="block", SUBSYSTEMS=="scsi", PROGRAM=="/lib/udev/scsi_id -g -u -d /dev/%k", RESULT==") . $scsi_id . q(", RUN+="/bin/sh -c ');
            print FILE q(/bin/echo ) . $max_sectors_kb . q( > /sys/block/%k/queue/max_sectors_kb; );
            print FILE q(/bin/echo ) . $read_ahead_kb  . q( > /sys/block/%k/queue/read_ahead_kb; );
            print FILE q(/bin/echo ) . $nr_requests    . q( > /sys/block/%k/queue/nr_requests; );
            print FILE q(/bin/echo ) . $queue_depth    . q( > /sys/block/%k/device/queue_depth; );
            print FILE q(/bin/echo ) . $elevator       . q( > /sys/block/%k/queue/scheduler; );
            print FILE qq('"\n);
            print FILE qq(#\n);
            print FILE qq(#  Multipathed device\n);
            print FILE q(SUBSYSTEM=="block", KERNEL=="dm-*", PROGRAM=="/lib/udev/scsi_id -g -u -d /dev/%k", RESULT==") . $scsi_id . q(", RUN+="/bin/sh -c ');
            print FILE q(/bin/echo ) . $max_sectors_kb . q( > /sys/block/%k/queue/max_sectors_kb; );
            print FILE q(/bin/echo ) . $read_ahead_kb  . q( > /sys/block/%k/queue/read_ahead_kb; );
            print FILE q(/bin/echo ) . $nr_requests    . q( > /sys/block/%k/queue/nr_requests; );
            print FILE q(/bin/echo ) . $queue_depth    . q( > /sys/block/%k/device/queue_depth; );
            print FILE q(/bin/echo ) . $elevator       . q( > /sys/block/%k/queue/scheduler; );
            print FILE qq('"\n\n);
            
        } else {
            print "$disk_name -- Couldn't retrieve the scsi_id from $server:$disk_path.\n";
            $ERROR++;
        }
    }

    #
    # Augment with device specific "always appropriate" settings
    #
    print FILE << 'EOF';
# ---------------------------------------------------------------------------
# The following is excerpted from "Implementing IBM FlashSystem 840" Redbook.
# ---------------------------------------------------------------------------
# 
#   Linux tuning
#   
#   The Linux kernel buffer file system writes data before it sends the data to the storage system.
#   With the FlashSystem 840, better performance can be achieved when the data is not buffered
#   but directly sent to the FlashSystem 840. When setting the scheduling policy to no operation
#   (NOOP), the fewest CPU instructions possible are used for each I/O. Setting the scheduler to
#   NOOP gives the best write performance on Linux systems. You can use the following setting
#   in most Linux distributions as a boot parameter: elevator=noop.
#   
#   Current Linux devices are managed by the device manager Udev. You can define how Udev
#   will manage devices by adding rules to the /etc/udev/rules.d directory. Example 5-24
#   shows a rule for the FlashSystem 840.
#   
#   Example 5-24 Linux device rules
#   
# [root@flashnsd1 ~]# cat 99-IBM-FlashSystem.rules
#   
ACTION=="add|change", SUBSYSTEM=="block", ATTRS{device/model}=="FlashSystem", ATTR{queue/scheduler}="noop", ATTR{queue/rq_affinity}="2", ATTR{queue/add_random}="0", ATTR{device/timeout}="5"
ACTION=="add|change", KERNEL=="dm-*", PROGRAM="/bin/bash -c 'cat /sys/block/$name/slaves/*/device/model | grep FlashSystem'", ATTR{queue/scheduler}="noop", ATTR{queue/rq_affinity}="2", ATTR{queue/add_random}="0"
#   
ACTION=="add|change", SUBSYSTEM=="block", ATTRS{device/model}=="FlashSystem-9840", ATTR{queue/scheduler}="noop", ATTR{queue/rq_affinity}="2", ATTR{queue/add_random}="0", ATTR{device/timeout}="10"
ACTION=="add|change", KERNEL=="dm-*", PROGRAM="/bin/bash -c 'cat /sys/block/$name/slaves/*/device/model | grep FlashSystem-9840'", ATTR{queue/scheduler}="noop", ATTR{queue/rq_affinity}="2", ATTR{queue/add_random}="0"
EOF
    
    close(FILE);

    return 1;
}

sub report_results {

    if($ERROR == 0) {

        print "Your new rules can be found here:\n\n  $out_file\n\n";
    
        if(defined $deploy) {
            distribute_rule($out_file);
            udev_reread();
        } else {
            print "\n";
            print "Not deploying.  You might want to try --deploy.\n";
            print "\n";
            print "See $progname --help for details.\n";
            print "\n";
        }
    
    } else {
        print "\n";
        print "  ERROR: Please make sure that all devices, and ideally all\n";
        print "         NSD servers are fully operational, then try again.\n";
        print "\n";
        print "         Try 'mmlsnsd -m' to verify.\n";
        print "\n";
        exit 1;
    }

    return 1;
}


sub get_tmp_file {

    my $cmd = "mktemp /tmp/$progname.99-gpfs-${fs}.rules.XXX";

    my $file;

    open(INPUT,"$cmd|") or die("Couldn't run $cmd for input");
    while(<INPUT>) {
        if( m#^(/tmp/$progname.*)# ) {
            $file = $1;
        }
    }
    close(INPUT);

    print ">> get_tmp_file => $file\n" if(defined $debug);
    return $file;
}

sub distribute_rule {

    my $file = shift;

    my $udev_rules_file = $udev_rules_file_base . "-${fs}.rules";

    print "Deploying as $udev_rules_file to:\n";
    foreach my $server (sort keys %servers) {
        print "  $server\n";
        my $cmd = "scp -q $file $server:$udev_rules_file";
        print ">> Command: $cmd\n" if(defined $debug);
        !system($cmd) or die("FAILED: $cmd\n");
    }
    print "\n";
    print "Done!  Your file system has been tuned and is ready for action.\n";
    print qq(Perhaps try "test_block_device_settings" on each of your NSD servers\n);
    print "\n";

    return 1;
}

sub udev_reread {

    #
    #   RHEL6 and friends, Ubuntu 12.10 and later:
    #       udevadm trigger --verbose --subsystem-match=block
    #       which udevadm >/dev/null 2>&1 && udevadm trigger --verbose --subsystem-match=block
    #
    #   RHEL5 and friends:
    #       udevcontrol reload_rules
    #
    foreach my $server (sort keys %servers) {
        my $cmd = qq(ssh $server 'which udevadm >/dev/null 2>&1  &&  udevadm trigger --subsystem-match=block  ||  udevcontrol reload_rules');
        print ">> Command: $cmd\n" if(defined $debug);
        !system($cmd) or die("FAILED: $cmd\n");
    }

    return 1;
}

sub get_scsi_id {
    
    my $server  = shift;
    my $disk_path   = shift;
    my $scsi_id;
    
    print "> get_scsi_id()\n" if(defined $debug);
    my $cmd = "ssh $server /lib/udev/scsi_id -g -u -d $disk_path";
    print ">> Command: $cmd\n" if(defined $debug);
    open(INPUT,"$cmd|") or die("Couldn't run $cmd for input.");
    while(<INPUT>) {
        # 
        #   Example Input -- this is what we expect the response to normally
        #   look like, but as long as it's a unique string, we don't really 
        #   care.  
        #
        #   DCS3700 style:
        #
        #       360080e50002934b80000403b5283dbe6
        #       360080e50002937b800003f6b5283dbf5
        #       360080e50002937b800003f8f5283dc3f
        # 
        #   FlashSystems 820 style:
        #
        #      20020c24001146a2c
        #      20020c24000146a2c
        #      20020c24002146a2c
        #
        if( m/^(\S+)/ ) {
            $scsi_id = $1;
            last;
        }
    }
    close(INPUT);

    if(defined $scsi_id) {
        return $scsi_id;
    } else {
        return undef;
    }
}


sub calculate_read_ahead_kb {

    my $block_allocation_type   = shift;
    my $max_sectors_kb          = shift;

    #
    # General best practice: 
    #
    #   If block allocation type is 'scatter', then set to zero.  If
    #   it's 'cluster', then set to /* XXX ask Ray for method to determine this */
    #
    #   -Tuning input from:
    #       Sven Oehme
    #       Ray Paden
    #
    if( "$block_allocation_type" eq 'scatter' ) {
        return 0;
    } else {
        # Hmm.  Must be type 'cluster'
        #XXX Ray -- what formula should we use here?  Using $max_sectors_kb for now... -BEF-
        return $max_sectors_kb;
    }
}


sub calculate_max_sectors_kb {

    my $block_size = shift;

    #
    #   GPFS block size / 4 or 512, whichever is greater.
    #
    #   max_sectors_kb should always be set as high as you can,
    #   independent of the block size.  However, empirical testing
    #   indicates lower performance, and in rare cases SCSI underruns,
    #   if this is set too high with certain hardware.
    #
    #   Therefore, we start with the conservative setting below, that
    #   should still be an improvement over the out of the box setup.
    #
    #   -Tuning input from:
    #       Sven Oehme
    #       Ray Paden
    #
    #   Note: GPFS block size is in bytes, so we must divide by 1024 to
    #   get kilobytes.  Then we divide by 4 to get our max_sectors_kb
    #   value.  Thanks go to Christian Caruthers for his input on this
    #   -BEF-.
    #
    my $kb = $block_size / 1024 / 4;

    if( $kb < 512 ) {
        $kb = 512;
    }

    return $kb;
}


sub get_disk_pool_info {

    print "> get_disk_pool_info()\n" if(defined $debug);
    my $cmd = "/usr/lpp/mmfs/bin/mmlsdisk $fs 2>&1";
    print ">> Command: $cmd\n" if(defined $debug);
    open(INPUT,"$cmd|") or die("Couldn't run $cmd for input.");
    while(<INPUT>) {
        #
        #   [root@flashnsd1 ~]# mmlsdisk tiered
        #   disk         driver   sector     failure holds    holds                            storage
        #   name         type       size       group metadata data  status        availability pool
        #   ------------ -------- ------ ----------- -------- ----- ------------- ------------ ------------
        #   flashctl1ab_lun0 nsd         512          -1 Yes      Yes   ready         up           system       
        #   flashctl1ab_lun1 nsd         512          -1 Yes      Yes   ready         up           system       
        #   flashctl1ab_lun2 nsd         512          -1 Yes      Yes   ready         up           system       
        #   flashctl1ab_lun3 nsd         512          -1 Yes      Yes   ready         up           system       
        #   sur_dcs3700a_lun0 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700b_lun1 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700a_lun2 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700b_lun3 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700a_lun4 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700b_lun5 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700a_lun6 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700b_lun7 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700a_lun8 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700b_lun9 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700a_lun10 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700b_lun11 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700a_lun12 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700b_lun13 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700a_lun14 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700b_lun15 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700a_lun16 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700b_lun17 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700a_lun18 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700b_lun19 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700a_lun20 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700b_lun21 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700a_lun22 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700b_lun23 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700a_lun24 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700b_lun25 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700a_lun26 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700b_lun27 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700a_lun28 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700b_lun29 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700a_lun30 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700b_lun31 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700a_lun32 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700b_lun33 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700a_lun34 nsd         512         121 No       Yes   ready         up           Tier2        
        #   sur_dcs3700b_lun35 nsd         512         121 No       Yes   ready         up           Tier2        

        print ">> $_" if(defined $debug);

        next if(m/^(disk|name|----+)\s/);
            #
            #   disk         driver   sector     failure holds    holds                            storage
            #   name         type       size       group metadata data  status        availability pool
            #   ------------ -------- ------ ----------- -------- ----- ------------- ------------ ------------
            #

        if(m/^(\S+)\s+.*\s+(\S+)/) {
            #  ^^^          ^^^
            #   |            |
            #   |            ----< pool
            #   ----< disk_name
    
            my $disk_name   = $1;
            my $pool        = $2;

            $disk_pool_by_disk_name{$disk_name} = $pool;
            $disk_pools{$pool} = 1;
        }
    }

    return 1;
}


sub calculate_nr_requests {

    my $disk_name = shift;

    #
    # nr_requests can be higher than queue_depth without much negative
    # impact as the device queue determines how much is actually queued
    # on the device.
    #
    # queue_depth is what can really hurt drive performance if it's set
    # too high.
    #
    # So, 32 per physical disk is a good starting point for nr_requests
    # and a queue depth of 4 per physical disk.
    #
    #   -Tuning input from:
    #       Sven Oehme
    #

    my $pool = $disk_pool_by_disk_name{$disk_name};
    my $nr_requests = $disks_per_array_by_pool{$pool} * 32;

    return $nr_requests;
}


sub calculate_queue_depth {

    my $disk_name = shift;

    #
    # Let's take an 8+2p array (8 data disks, 2 parity disks, AKA RAID6)
    # as an example:
    #
    #   The most accurate value for _writes_ would be based on 8 disks.
    #   I/O on the other 2 disks is "generated parity" by the array, and
    #   need not be considered by the OS.
    #
    #   The most accurate value for _reads_ would be based on 10 disks.
    #   Reads may pull data from across all 10 disks.
    #
    #   Therefore, we recommend the conservative approach for this tool,
    #   which would be 8.
    #
    #   -Tuning input from:
    #       Sven Oehme
    #

    my $pool = $disk_pool_by_disk_name{$disk_name};
    my $queue_depth = $disks_per_array_by_pool{$pool} * 4;

    return $queue_depth;
}


sub version {
    print qq(\n);
    print qq($progname v$version_number\n);
    print qq(\n);
    print qq(    Part of the "gpfs_goodies" package\n);
    print qq(\n);

    return 1;
}

sub usage {

    my $udev_rules_file = $udev_rules_file_base . "-FSNAME.rules";

version();
print << "EOF";
Usage:  $progname --filesystem FSNAME [OPTION...]

    $progname should be considered BETA code at this point.  
           
    All options can be abbreviated to minimum uniqueness.

    This program will examine your environment including GPFS, storage
    servers, disk subsystems, and LUNs, to calculate best practice block
    device tuning settings.  It will create one udev rules file per file
    system with the new tuning settings, using one entry for each LUN,
    and optionally deploy it to each participating storage server.

    This tool is intended to be run with GPFS 'active' on all the NSD
    servers serving your specified file system.

    Note that it will skip over and ignore any GSS file systems.  GSS block
    device settings are tuned directly by the GSS software stack.
    
    --help

    --version

    --filesystem FSNAME

        Where FSNAME is the name of the file system whose block devices
        should be tuned.


    --disks-per-array (NN|NN:POOLNAME,[...])

        Where NN is the number of disks in each array.  
        
        Often, each LUN presented to the OS represents one RAID array in
        the storage subsystem.  Here are some examples by array type:

          Value for NN    Array Type
          ------------    --------------------------------------------
               8          8+2p RAID6        (8 data + 2 parity disks)
               8          8+3p Reed Solomon (8 data + 3 parity disks)
               4          4+1p RAID5        (4 data + 1 parity disk)
               1          1+1m RAID1        (1 data + 1 mirrored disk)

        Hint #1:  If all of your disks are in the same kind of array
        (e.g.: RAID6 with 8 data disks + 2 parity disks), then you can
        simply use the "NN" format, even if you have multiple file
        systems and multiple pools.

        Hint #2:  If you don't know what all this "pool" stuff is about,
        then you probably only have one pool (the "system" pool).  Try
        "mmlspool FSNAME" to have a look if you're curious.

        NN - If only "NN" is specified, then it is assumed that NN
        represents the number of disks per array across all file systems
        and living on all pools served by "--servers" (or you only have
        one pool per file system).

        NN:POOLNAME - The number of disks (NN) per array across all
        arrays in pool POOLNAME that are part of file system FSNAME.

        Examples:

            --disks-per-array 8
            --disks-per-array 4:system,8:tier2

        Default: $disks_per_array_default


    --test

        Create the rules, but don't deploy them.  This is the default
        action if --deploy is not specified.


    --deploy

        Deploy and activate the resultant udev rules file to each
        participating storage server.  Participating storage servers are
        identified by their role as an NSD server for any of the LUNs in
        active file systems.  Execute the command "mmlsnsd" for a list
        of these servers.
        
        The name of the udev rules file on the target NSD servers will
        be: $udev_rules_file


    --out-file FILE

        The name of your resultant udev rules file.  This file can be
        given any name you like.  

        If you also use the --deploy option, this file will still be 
        deployed to your storage servers with the name of:
        
            $udev_rules_file

        Example:  --out-file /tmp/my_shiny_new_udev_rules_file.conf

        Default:  I'll choose one for you and tell you what I've named
        it.


    Support: 
    
        This software is provided as-is, with no express or implied
        support.  However, the author would love to receive your
        patches.  Please contact Brian E. Finley <bfinley\@us.ibm.com>
        with patches and/or suggestions.

EOF
    return 1;
}

sub user_is_root {

    if($< == 0) {
        return 1;
    }
    return undef;
}


#
#   END Subroutines
#
########################################################################

#
#   :set tw=0 ts=4 ai et   
#
