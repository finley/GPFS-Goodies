- Add a "--scheduler [cfq|noop|deadline|as]" option to
  tune_block_device_settings...


- Comments from Scott Denham:

    In the longer run, I'd like to see it be more "Linux-like" in storing
    the specific parameter values in /etc/sysconfig/<device type> or the
    like, and to be able to dynamically expand / contract which
    /sys/block/<dev>/ paramters are tuned, as opposed to storing the values
    directly in the scripts.  This would allow updating without
    re-customizing, and puts the tunables in a logical place instead of
    buried inside /etc/udev/rules. 
    
    I like your idea about probing the local Ethernet for candidates for
    management interfaces for storage devices.  The possible concerns here
    would be time, should this be a system with a large, flat Layer2
    management net, or reachability should the storage devices actually be
    on a separate, routeable subnet from the GPFS system where gpfs_goodies
    is being run. Perhaps a simple old-xCAT style "tab" with IP:DEVICE TYPE
    as a complement to the discovery logic?
    

- Incorporate "Net::SSH::Expect" into this code.  It does not appear to
  be commonly available in either Debian or Red Hat based distros.

    http://search.cpan.org/~bnegrao/Net-SSH-Expect-1.09/lib/Net/SSH/Expect.pod

- Which of these should we use in a multi-node GPFS environment?

    no_path_retry           15

        or 

    no_path_retry           fail


- In the nsddevices file, is this statement true?

   # 
   # dmm vs. generic is used by GPFS to prioritize internal order of
   # searching through available disks, then later GPFS discards other
   # disk device names that it finds that match as the same NSD device
   # by a different path.  For this reason, dmm vs. generic is an
   # important distinction if you are not explicitly producing the
   # entire and exclusive set of disks that GPFS should use, as output
   # from this script (nsddevices) _and_ exiting this script with a
   # "return 0". -Brian Finley
   #

--

- Add "HOWTO Convert from LSI RDAC to Linux Multipath with GPFS Goodies"

- create a new tool

    - tune_mmrestripefs_bandwidth

        - This tool gives you a way to specify the rate at which your
          data migration operation proceeds.  For example, you can use
          it to achieve a slow migration over an extended time frame
          to avoid impacting performance for production operations.
          
          When used without this tool, mmrestripefs will consume all the
          bandwidth it can to complete it's operation as fast as
          possible, limited only by the performance of the participating
          nodes' network connectivity and/or the storage server disk
          performance.  This can result in noticably lower performance
          of the file system for users and/or other applications.

        - Uses iptables to impose a bandwidth limit on rate of re-stripe
          activity.

        OPTIONS:
            -h
            -v
            --MBps NUMBER

                Limit the bandwidth to a maximum of this many megabytes
                per second.

            --Nodes NODE1[,NODE2,...]

                The list of nodes that will be used for this operation.
                
                If multiple nodes are included in this list, the 
                bandwidth specified via the "--MBps" option will be
                divided equally among them, so that the aggregate
                bandwidth stays within that limit.
                
            --mmrestripefs-options "OPTION1[,OPTION2,...]"
                
                A list options, within quotation marks, that should be
                passed to the mmrestripefs command.  The -N option can
                be excluded and will be ignored if it is included.

                Example: 

                --mmrestripefs-options "fs1 -m"


- prio_callout vs. prio

- Add "man gpfs_goodies"

- sort multipath.conf LUN entires same way as test_block_devices

- use SMcli to determine alua vs. rdac
    - provide warning if not recommended setting by controller type
    - consider adding settings that are unique by controller instance
      (ie.: one 1818 w/alua and one 1818 with Linux).
    - consider adding host specific settings:
        ie.: one host w/luns presented as alua and another with the same
        luns presented as Linux

- add option "--sm-password PASSWORD" 

Blacklist

    LSSCSI output for devices to blacklist:

    [0:2:0:0]    disk    IBM      ServeRAID M5015  2.12  /dev/sda 
    [4:0:0:0]    cd/dvd  HL-DT-ST DVDRAM GT30N     IS09  /dev/sr0 

  devnode "^usbsd*"

  device {
     vendor                "ServeRA"
     product               *
  }
  device {
     vendor                "LSILOGIC"
     product               "Logical Volume"
  }
  device {
     vendor                "IBM-ESXS"
     product               *
  }

        device {
                vendor  "ServeRA"
                product "8k-l Mirror"
        }
        device {
                vendor  "IBM"
                product "VirtualDisk"
       




LSSCSI output for devices to handle:

    [3:0:0:0]    disk    IBM      1813      FAStT  0786  /dev/sdb 
    [3:0:0:1]    disk    IBM      1813      FAStT  0786  /dev/sdc 
    [3:0:0:2]    disk    IBM      1813      FAStT  0786  /dev/sdd 




FAQ

    Where do I need to run multipath.conf-creator?

    A. Any node that can use SMclient to connect to the storage
    subsystems.


    Where do I need to copy the multipath.conf files?

    A. All storage nodes.



Make sure that multipathd is running

Make to sort the output numerically (ie. lun2 comes before lun10)

FOr the RPM, change paths in the HOWTO for files copied.





CONTRIBUTORS

    Ray Paden
    Scott Denham


+# 
+#   TODO: perhaps add some notes on using these commands for additional
+#   verification? -BEF-
+# 
+#       - mmnsddiscover -a
+#       - mmlsdisk gpfs1 -M
+#

--

Ray Paden
I have a suggestion... you write... 
11:10:35 AM
◄
# WARNING! Make sure you have performed an "mmshutdown" on each NSD
# server prior to copying out the new file and doing the reconfigure
# below. If there are any differences between this config and your
# prior config, taking that precaution will ensure you don't provoke
# any corruption in any pre-existing GPFS filesystems.
 
 11:10:45 AM
 Brian Finley
 k 
 11:10:58 AM
 Ray Paden
 I think it is OK to say this instead... 
 11:11:54 AM
 ◄
 # WARNING: Unmount any file system and delete any NSDs corresponding to
 # existing LUNs 
 11:12:09 AM
 ◄
 It probably needs a little more detail, but you get the idea. 
 11:12:53 AM
 ◄
 One of the thing we need to be careful about is if they are adding new
 LUNs to an existing cluster.  
 11:12:56 AM
 Brian Finley
 ok, I'll add that to my notes 
 11:13:18 AM
 ◄
 right -- they may not always need to delete existing LUNs... 

--

